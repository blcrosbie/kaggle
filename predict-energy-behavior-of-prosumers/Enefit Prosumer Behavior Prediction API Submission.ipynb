{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b59702",
   "metadata": {},
   "source": [
    "# Enefit - Predict Energy Behavior of Prosumers\n",
    "<h4>Predict Prosumer Energy Patterns and Minimize Imbalance Costs:</h4>\n",
    "<p>Your challenge in this competition is to predict the amount of electricity produced and consumed by Estonian energy customers who have installed solar panels. You'll have access to weather data, the relevant energy prices, and records of the installed photovoltaic capacity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c471c563-5071-49c3-82c8-d23f4f6ba1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import holidays\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import joblib\n",
    "from joblib import load\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "# import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6def50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Local\n",
    "import public_timeseries_testing_util as enefit\n",
    "\n",
    "# # For Live\n",
    "# import enefit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db6bcd-75a0-4988-b243-a56a60a581d0",
   "metadata": {},
   "source": [
    "### Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c85f62-5918-47a1-a813-579c575d1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "    # root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "    root = os.getcwd()\n",
    "\n",
    "    data_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "        \"row_id\",\n",
    "    ]\n",
    "    client_cols = [\n",
    "        \"product_type\",\n",
    "        \"county\",\n",
    "        \"eic_count\",\n",
    "        \"installed_capacity\",\n",
    "        \"is_business\",\n",
    "        \"date\",\n",
    "    ]\n",
    "    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n",
    "    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n",
    "    forecast_weather_cols = [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"hours_ahead\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"cloudcover_high\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_total\",\n",
    "        \"10_metre_u_wind_component\",\n",
    "        \"10_metre_v_wind_component\",\n",
    "        \"forecast_datetime\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"surface_solar_radiation_downwards\",\n",
    "        \"snowfall\",\n",
    "        \"total_precipitation\",\n",
    "    ]\n",
    "    historical_weather_cols = [\n",
    "        \"datetime\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"rain\",\n",
    "        \"snowfall\",\n",
    "        \"surface_pressure\",\n",
    "        \"cloudcover_total\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_high\",\n",
    "        \"windspeed_10m\",\n",
    "        \"winddirection_10m\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ]\n",
    "    location_cols = [\"longitude\", \"latitude\", \"county\"]\n",
    "    target_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_data = pl.read_csv(\n",
    "            os.path.join(self.root, \"train.csv\"),\n",
    "            columns=self.data_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_client = pl.read_csv(\n",
    "            os.path.join(self.root, \"client.csv\"),\n",
    "            columns=self.client_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_gas_prices = pl.read_csv(\n",
    "            os.path.join(self.root, \"gas_prices.csv\"),\n",
    "            columns=self.gas_prices_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_electricity_prices = pl.read_csv(\n",
    "            os.path.join(self.root, \"electricity_prices.csv\"),\n",
    "            columns=self.electricity_prices_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_electricity_prices = self.df_electricity_prices.drop_nulls()\n",
    "        \n",
    "        self.df_forecast_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"forecast_weather.csv\"),\n",
    "            columns=self.forecast_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_historical_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"historical_weather.csv\"),\n",
    "            columns=self.historical_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_weather_station_to_county_mapping = pl.read_csv(\n",
    "            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n",
    "            columns=self.location_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_data = self.df_data.filter(\n",
    "            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n",
    "        )\n",
    "        self.df_target = self.df_data.select(self.target_cols)\n",
    "\n",
    "        self.schema_data = self.df_data.schema\n",
    "        self.schema_client = self.df_client.schema\n",
    "        self.schema_gas_prices = self.df_gas_prices.schema\n",
    "        self.schema_electricity_prices = self.df_electricity_prices.schema\n",
    "        self.schema_forecast_weather = self.df_forecast_weather.schema\n",
    "        self.schema_historical_weather = self.df_historical_weather.schema\n",
    "        self.schema_target = self.df_target.schema\n",
    "\n",
    "        self.df_weather_station_to_county_mapping = (\n",
    "            self.df_weather_station_to_county_mapping.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def update_with_new_data(\n",
    "        self,\n",
    "        df_new_client,\n",
    "        df_new_gas_prices,\n",
    "        df_new_electricity_prices,\n",
    "        df_new_forecast_weather,\n",
    "        df_new_historical_weather,\n",
    "        df_new_target,\n",
    "    ):\n",
    "        df_new_client = pl.from_pandas(\n",
    "            df_new_client[self.client_cols], schema_overrides=self.schema_client\n",
    "        )\n",
    "        df_new_gas_prices = pl.from_pandas(\n",
    "            df_new_gas_prices[self.gas_prices_cols],\n",
    "            schema_overrides=self.schema_gas_prices,\n",
    "        )\n",
    "        df_new_electricity_prices = pl.from_pandas(\n",
    "            df_new_electricity_prices[self.electricity_prices_cols],\n",
    "            schema_overrides=self.schema_electricity_prices,\n",
    "        )\n",
    "        df_new_forecast_weather = pl.from_pandas(\n",
    "            df_new_forecast_weather[self.forecast_weather_cols],\n",
    "            schema_overrides=self.schema_forecast_weather,\n",
    "        )\n",
    "        df_new_historical_weather = pl.from_pandas(\n",
    "            df_new_historical_weather[self.historical_weather_cols],\n",
    "            schema_overrides=self.schema_historical_weather,\n",
    "        )\n",
    "        df_new_target = pl.from_pandas(\n",
    "            df_new_target[self.target_cols], schema_overrides=self.schema_target\n",
    "        )\n",
    "\n",
    "        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n",
    "            [\"date\", \"county\", \"is_business\", \"product_type\"]\n",
    "        )\n",
    "        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n",
    "            [\"forecast_date\"]\n",
    "        )\n",
    "        self.df_electricity_prices = pl.concat(\n",
    "            [self.df_electricity_prices, df_new_electricity_prices]\n",
    "        ).unique([\"forecast_date\"])\n",
    "        self.df_forecast_weather = pl.concat(\n",
    "            [self.df_forecast_weather, df_new_forecast_weather]\n",
    "        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n",
    "        self.df_historical_weather = pl.concat(\n",
    "            [self.df_historical_weather, df_new_historical_weather]\n",
    "        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n",
    "        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n",
    "            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
    "        )\n",
    "\n",
    "    def preprocess_test(self, df_test):\n",
    "        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "        df_test = pl.from_pandas(\n",
    "            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n",
    "        )\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de0256c1-5cf3-426d-aada-9d70eb6f1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),)\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),(np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),(np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _client_features(self, df_features):\n",
    "        df_client = self.data.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),on=[\"county\", \"is_business\", \"product_type\", \"date\"],how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "#             .drop(\"hours_ahead\")\n",
    "            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n",
    "            .join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\"))\n",
    "\n",
    "        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=\"datetime\",how=\"left\",suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (self.data.df_weather_station_to_county_mapping)\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            ).join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (df_historical_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=\"datetime\",how=\"left\",suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),).filter(pl.col(\"hour\") <= 10).drop(\"hour\"),on=\"datetime\",how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _target_features(self, df_features):\n",
    "        df_target = self.data.df_target\n",
    "\n",
    "        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n",
    "\n",
    "        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n",
    "        \n",
    "        hours_list=[i*24 for i in range(2,15)]\n",
    "\n",
    "        for hours_lag in hours_list:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [f\"target_{hours_lag}h\" for hours_lag in hours_list[:4]]\n",
    "        \n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n",
    "            )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),(\"target\", 24 * 2, 24 * 9),(\"target\", 24 * 3, 24 * 10),(\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),(\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\"datetime\", \"hour\", \"dayofyear\")\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"segment\",]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    # added some new features here\n",
    "    def _additional_features(self,df):\n",
    "        for col in [\n",
    "                    'temperature', \n",
    "                    'dewpoint', \n",
    "                    '10_metre_u_wind_component', \n",
    "                    '10_metre_v_wind_component', \n",
    "            ]:\n",
    "            for window in [1]:\n",
    "                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n",
    "        return df\n",
    "    \n",
    "    def _log_outliers(self,df):\n",
    "        l1=['installed_capacity', 'target_mean', 'target_std']\n",
    "        for i in l1:\n",
    "            df = df.with_columns([(f\"log_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n",
    "        return df\n",
    "        \n",
    "\n",
    "    def generate_features(self, df_prediction_items,isTrain):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._general_features,self._client_features,self._forecast_weather_features,\n",
    "            self._historical_weather_features,self._target_features,self._holidays_features,\n",
    "            self._log_outliers,self._reduce_memory_usage,self._drop_columns,]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "        df_features = self._additional_features(df_features)\n",
    "\n",
    "        return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "175ef1d8-aa4b-4097-ac34-43ad492f874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesGenerator:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_date = (\n",
    "            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (\n",
    "            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "\n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24,3 * 24,4 * 24,5 * 24,6 * 24,7 * 24,8 * 24,9 * 24,10 * 24,11 * 24,12 * 24,13 * 24,14 * 24,]:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f\"target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),\n",
    "            (\"target\", 24 * 2, 24 * 9),\n",
    "            (\"target\", 24 * 3, 24 * 10),\n",
    "            (\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_target_features,\n",
    "            self._add_holidays_features,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns,\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11823d-d2f6-4e22-92fe-da1a5c6e287a",
   "metadata": {},
   "source": [
    "## initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f542ac89-97a3-4ac5-88a3-547ddc6d72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_storage = DataStorage()\n",
    "features_generator = FeaturesGenerator(data_storage=data_storage)\n",
    "feat_gen = FeatureEngineer(data=data_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41097cb-4632-4697-beed-a710458b47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features = features_generator.generate_features(data_storage.df_data)\n",
    "df_train_features = df_train_features[df_train_features['target'].notnull()]\n",
    "\n",
    "df_train = feat_gen.generate_features(data_storage.df_data,True)\n",
    "df_train = df_train[df_train['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf68d5b6-8b7a-4100-a481-34c32890ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e4fa04f-5e1c-4a39-b204-ff16f83bcc04",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_pandas() got an unexpected keyword argument 'strict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     df_test, \n\u001b[0;32m      3\u001b[0m     df_new_target, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     df_sample_prediction\n\u001b[0;32m     10\u001b[0m ) \u001b[38;5;129;01min\u001b[39;00m iter_test:\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mdata_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_with_new_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_gas_prices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_gas_prices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_electricity_prices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_electricity_prices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_forecast_weather\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_forecast_weather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_historical_weather\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_historical_weather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_target\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#separately generate test features for both models\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     df_test \u001b[38;5;241m=\u001b[39m data_storage\u001b[38;5;241m.\u001b[39mpreprocess_test(df_test)\n",
      "Cell \u001b[1;32mIn[11], line 143\u001b[0m, in \u001b[0;36mDataStorage.update_with_new_data\u001b[1;34m(self, df_new_client, df_new_gas_prices, df_new_electricity_prices, df_new_forecast_weather, df_new_historical_weather, df_new_target)\u001b[0m\n\u001b[0;32m    136\u001b[0m df_new_client \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mfrom_pandas(\n\u001b[0;32m    137\u001b[0m     df_new_client[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_cols], schema_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_client\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    139\u001b[0m df_new_gas_prices \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mfrom_pandas(\n\u001b[0;32m    140\u001b[0m     df_new_gas_prices[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgas_prices_cols],\n\u001b[0;32m    141\u001b[0m     schema_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_gas_prices,\n\u001b[0;32m    142\u001b[0m )\n\u001b[1;32m--> 143\u001b[0m df_new_electricity_prices \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_new_electricity_prices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melectricity_prices_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema_electricity_prices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m df_new_forecast_weather \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mfrom_pandas(\n\u001b[0;32m    149\u001b[0m     df_new_forecast_weather[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecast_weather_cols],\n\u001b[0;32m    150\u001b[0m     schema_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_forecast_weather,\n\u001b[0;32m    151\u001b[0m )\n\u001b[0;32m    152\u001b[0m df_new_historical_weather \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mfrom_pandas(\n\u001b[0;32m    153\u001b[0m     df_new_historical_weather[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistorical_weather_cols],\n\u001b[0;32m    154\u001b[0m     schema_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_historical_weather,\n\u001b[0;32m    155\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: from_pandas() got an unexpected keyword argument 'strict'"
     ]
    }
   ],
   "source": [
    "for (\n",
    "    df_test, \n",
    "    df_new_target, \n",
    "    df_new_client, \n",
    "    df_new_historical_weather,\n",
    "    df_new_forecast_weather, \n",
    "    df_new_electricity_prices, \n",
    "    df_new_gas_prices, \n",
    "    df_sample_prediction\n",
    ") in iter_test:\n",
    "\n",
    "    data_storage.update_with_new_data(\n",
    "        df_new_client=df_new_client,\n",
    "        df_new_gas_prices=df_new_gas_prices,\n",
    "        df_new_electricity_prices=df_new_electricity_prices,\n",
    "        df_new_forecast_weather=df_new_forecast_weather,\n",
    "        df_new_historical_weather=df_new_historical_weather,\n",
    "        df_new_target=df_new_target\n",
    "    )\n",
    "    \n",
    "    #separately generate test features for both models\n",
    "    \n",
    "    df_test = data_storage.preprocess_test(df_test)\n",
    "    \n",
    "    df_test_features = features_generator.generate_features(df_test)\n",
    "    \n",
    "    df_test_feats = feat_gen.generate_features(df_test,False)\n",
    "    \n",
    "    df_test_feats.drop(columns=['date','literal'],inplace=True)\n",
    "        \n",
    "    pred1 = predict(df_test_features)\n",
    "    \n",
    "    pred2 = predict_model(df_test_feats)\n",
    "    \n",
    "    # Ensembling with slightly tuned model weights\n",
    "    pred_1_w = 0.49\n",
    "    df_sample_prediction[\"target\"] = (\n",
    "        (pred_1_w * pred1) + \n",
    "        ((1 - pred_1_w) * pred2)\n",
    "    )\n",
    "    \n",
    "    env.predict(df_sample_prediction)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51102d8-574e-4e37-b05a-e3f845bdbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.read_csv(os.path.join(\"example_test_files\", \"revealed_targets.csv\"))\n",
    "p = preds[['row_id', 'data_block_id', 'target']]\n",
    "p['row_id'] = p['row_id'] + 6336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f4b53-f796-4616-812f-82aefb4a8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_init, y_init in iter_test:\n",
    "    print(env._status)\n",
    "    current_data_block_id = X_init.data_block_id.drop_duplicates().to_list()[0]\n",
    "    p_current = p[p.data_block_id == current_data_block_id]\n",
    "    # print(X_init)\n",
    "    # print(y_init)\n",
    "    # print(p_current)\n",
    "    \n",
    "    env.predict(p_current)\n",
    "    print(env._status)\n",
    "    env.iter_test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5ada3-6726-4374-963b-3909744e9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "env._status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4136e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import holidays\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,Dict,Tuple\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12210581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from solar_tracking import calculate_elevation_angle, calculate_irradiation_on_surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams.update(**{'figure.dpi': 150})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f2362",
   "metadata": {},
   "source": [
    "### Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTONIA_GPKG = \"estonia.gpkg\"\n",
    "\n",
    "# Coordinate Reference System (CRS)\n",
    "ESTONIA_CRS = \"EPSG:4133\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all spatial layers\n",
    "# boundry = gpd.read_file(ESTONIA_GPKG, layer='boundry', driver=\"GPKG\", crs=ESTONIA_CRS)\n",
    "counties = gpd.read_file(os.path.join(os.getcwd(), ESTONIA_GPKG), layer='counties', driver=\"GPKG\", crs=ESTONIA_CRS)\n",
    "# municipalities = gpd.read_file(ESTONIA_GPKG, layer='municipalities', driver=\"GPKG\", crs=ESTONIA_CRS)\n",
    "# settlements = gpd.read_file(ESTONIA_GPKG, layer='settlements', driver=\"GPKG\", crs=ESTONIA_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550da0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the counties of Estonia as well as the centroids to those counties\n",
    "fig, ax = plt.subplots(figsize=(6,8))\n",
    "counties.plot(ax=ax)\n",
    "counties.centroid.plot(markersize=12,color='red',ax=ax)\n",
    "plt.axis('off')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea45256",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('historical_weather.csv', parse_dates=['datetime'])\n",
    "forecast = pd.read_csv('forecast_weather.csv', parse_dates=['origin_datetime', 'forecast_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['utc_offset'] = weather.datetime.apply(handle_timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast['utc_offset'] = forecast.origin_datetime.apply(handle_timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddada5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_gdf = gpd.GeoDataFrame(\n",
    "    weather, geometry=gpd.points_from_xy(weather.longitude, weather.latitude), crs=ESTONIA_CRS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_gdf = gpd.GeoDataFrame(\n",
    "    forecast, geometry=gpd.points_from_xy(forecast.longitude, forecast.latitude), crs=ESTONIA_CRS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c970cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_data = gpd.overlay(counties,\\\n",
    "                          weather_gdf,\\\n",
    "                          how='intersection',\\\n",
    "                          keep_geom_type=False\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_pred = gpd.overlay(counties,\\\n",
    "                          forecast_gdf,\\\n",
    "                          how='intersection',\\\n",
    "                          keep_geom_type=False\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68547b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_min = weather[weather.direct_solar_radiation > 0].direct_solar_radiation.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d958a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_max = weather.direct_solar_radiation.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce17bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_time = train_df['datetime'].min()\n",
    "latest_time = train_df['datetime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2909c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0932d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_p(values, k):\n",
    "    \"\"\"\n",
    "    Given a list of values and `k` bins,\n",
    "    returns a list of their Maximum P bin number.\n",
    "    \"\"\"\n",
    "    binning = MaxP(values, k=k)\n",
    "    return binning.yb\n",
    "\n",
    "\n",
    "def bin_data(values, k, interval=\"equal\"):\n",
    "    \"\"\"\n",
    "    Given a list of values, 'k' bin\n",
    "    return categorical bin group ids in specified intervals\n",
    "    \"\"\"\n",
    "    min_val = values.min()\n",
    "    max_val = values.max()\n",
    "    \n",
    "    binned_data = []\n",
    "    thresholds = []\n",
    "    \n",
    "    if interval == \"equal\":\n",
    "        intv = (max_val - min_val) / k\n",
    "        thresholds = [i*intv for i in range(k)]\n",
    "        \n",
    "    elif interval == \"stdev\":\n",
    "        intv = values.std()\n",
    "        avg = values.mean()\n",
    "        if k % 2 == 0:\n",
    "            k_half = int(k/2)\n",
    "            k_res = 0\n",
    "        else:\n",
    "            k_half = int((k-1)/2)\n",
    "            k_res = 0.5\n",
    "            \n",
    "        for i in range(-1*k_half, k_half+1):\n",
    "            if i == 0 and k_res == 0:\n",
    "                pass\n",
    "            else:\n",
    "                thresholds.append((i*intv) + avg)\n",
    "\n",
    "    elif interval == \"equal_count\":\n",
    "        pass\n",
    "        \n",
    "    elif interval == \"quantile\":\n",
    "        pass\n",
    "    \n",
    "    elif interval == \"log_scale\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    binned_data = []\n",
    "    for val in values:\n",
    "        for j in range(0, len(thresholds)):\n",
    "            if val < thresholds[j]:\n",
    "                binned_data.append(j)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    return pd.Series(binned_data)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def extract_dt_attributes(df: pd.DataFrame):\n",
    "    # convert datetime column, if not done already\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # dates and times\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['time'] = df['datetime'].dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    #\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['day_of_year'] = df['datetime'].dt.strftime('%j').astype(int)\n",
    "    df['datediff_in_days'] = (\n",
    "        df['datetime']- earliest_time\n",
    "    ).dt.days\n",
    "    \n",
    "    # dictionary with time features as keys\n",
    "    # and min and max as values\n",
    "    time_features = {\n",
    "        'hour': [0, 23],\n",
    "        'dayofweek': [0, 6],\n",
    "        'week': [1, 52],\n",
    "        'month': [1, 12]\n",
    "    }\n",
    "    \n",
    "    for col in time_features:\n",
    "        if col=='week':\n",
    "            df[col] = df['datetime'].dt.isocalendar().week.astype(np.int32)\n",
    "        else:\n",
    "            df[col] = getattr(df['datetime'].dt,col)\n",
    "        \n",
    "        \n",
    "        ## sin and cosine features to capture the circular continuity\n",
    "        col_min,col_max = time_features[col]\n",
    "        angles = 2*np.pi*(df[col]-col_min)/(col_max-col_min+1)\n",
    "        \n",
    "        # add sin and cos\n",
    "        df[col+'_sine'] = np.sin(angles).astype('float')\n",
    "        df[col+'_cosine'] = np.cos(angles).astype('float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_timezone(dt):\n",
    "    \"\"\"\n",
    "    https://www.timeanddate.com/time/change/estonia/tallinn\n",
    "    \"\"\"\n",
    "    year = dt.year\n",
    "    \n",
    "    if year == 2021:\n",
    "        \n",
    "        if dt >= datetime.datetime.strptime(\"2021-10-31 03:00:00\", '%Y-%m-%d %H:%M:%S'):\n",
    "            utc_offset = 2\n",
    "        else:\n",
    "            utc_offset = 3\n",
    "        \n",
    "        \n",
    "    elif year == 2022:\n",
    "        \n",
    "        if dt < datetime.datetime.strptime(\"2022-03-27 03:00:00\", '%Y-%m-%d %H:%M:%S'):\n",
    "            utc_offset = 2\n",
    "        elif dt >= datetime.datetime.strptime(\"2022-10-30 03:00:00\", '%Y-%m-%d %H:%M:%S'):\n",
    "            utc_offset = 2\n",
    "        else:\n",
    "            utc_offset = 3\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif year == 2023:\n",
    "        \n",
    "        if dt < datetime.datetime.strptime(\"2023-03-26 03:00:00\", '%Y-%m-%d %H:%M:%S'):\n",
    "            utc_offset = 2\n",
    "            \n",
    "        elif dt >= datetime.datetime.strptime(\"2023-10-29 03:00:00\", '%Y-%m-%d %H:%M:%S'):\n",
    "            utc_offset = 2\n",
    "        \n",
    "        else:\n",
    "            utc_offset = 3\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return utc_offset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbff953",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dt_attributes(county_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadeb4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_month = 1\n",
    "# monthly_avg = county_data[(county_data.month == this_month) & (county_data.direct_solar_radiation > 0.0)].groupby(by=['countyCodes']).mean()\n",
    "# monthly_max = county_data[(county_data.month == this_month) & (county_data.direct_solar_radiation > 0.0)].groupby(by=['countyCodes']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_avg = pd.merge(monthly_avg, counties, on='countyCodes')\n",
    "# monthly_max = pd.merge(monthly_max, counties, on='countyCodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db39243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# county_data['direct_solar_bin'] = bin_data(county_data['direct_solar_radiation'], k=6, interval=\"stdev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for month in range(1, 13):\n",
    "\n",
    "#     monthly_avg = county_data[(county_data.month == month) & (county_data.direct_solar_radiation > 0.0)].groupby(by=['countyCodes']).median()\n",
    "#     monthly_max = county_data[(county_data.month == month) & (county_data.direct_solar_radiation > 0.0)].groupby(by=['countyCodes']).max()\n",
    "#     monthly_avg = pd.merge(monthly_avg.reset_index(), counties, on='countyCodes')\n",
    "#     monthly_max = pd.merge(monthly_max.reset_index(), counties, on='countyCodes')\n",
    "\n",
    "#     fig, ax = plt.subplots(2, 1, sharex=True, sharey=True)\n",
    "\n",
    "#     divider_0 = make_axes_locatable(ax[0])\n",
    "#     divider_1 = make_axes_locatable(ax[1])\n",
    "\n",
    "#     cax_0 = divider_0.append_axes(\"top\", size=\"5%\", pad=0.5)\n",
    "#     cax_1 = divider_1.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "\n",
    "#     monthly_avg.plot(\n",
    "#         column=\"direct_solar_radiation\",\n",
    "#         ax=ax[0],\n",
    "#         legend=True,\n",
    "#         cax=cax_0,\n",
    "#         legend_kwds={\"label\": f\"Nom Direct Solar Radiation: Month {month}\", \"orientation\": \"horizontal\"},\n",
    "#         cmap='OrRd'\n",
    "#     );\n",
    "#     monthly_max.plot(\n",
    "#         column=\"direct_solar_radiation\",\n",
    "#         ax=ax[1],\n",
    "#         legend=True,\n",
    "#         cax=cax_1,\n",
    "#         legend_kwds={\"label\": f\"Max Direct Solar Radiation: Month {month}\", \"orientation\": \"horizontal\"}\n",
    "#     );\n",
    "    \n",
    "    \n",
    "#     print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast[forecast.hours_ahead == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e64d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = pd.read_csv('weather_station_to_county_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd9138-1bef-4a05-87a1-bc1477a4a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('county_id_to_name_map.json', 'r') as fw:\n",
    "    county_lookup = json.load(fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecbf98",
   "metadata": {},
   "source": [
    "### Solar Production Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51dfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = train_df[train_df['is_consumption'] == 0]\n",
    "spa = spa.rename(columns={'county': 'countyCodes'})\n",
    "spa['countyCodes'] = spa['countyCodes'].astype(str)\n",
    "spa = spa.set_index('countyCodes', 'datetime')\n",
    "county_data['countyCodes'] = county_data['countyCodes'].astype(str)\n",
    "county_weather_data = county_data.set_index('countyCodes', 'datetime')\n",
    "spa = pd.merge(spa, county_weather_data, on=['countyCodes', 'datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ad2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_columns_spa = ['countyCodes', 'datetime', 'is_business', 'product_type', 'prediction_unit_id', 'target', \n",
    "                      'direct_solar_radiation', 'diffuse_radiation', 'shortwave_radiation',\n",
    "                      'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure', 'cloudcover_total',\n",
    "                      'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n",
    "                      'dayofweek', 'dayofweek_sine', 'dayofweek_cosine', 'week', 'week_sine', 'week_cosine',\n",
    "                      'month', 'month_sine', 'month_cosine', 'day_of_year', 'latitude', 'longitude']\n",
    "\n",
    "x_columns = [col for col in filter_columns_spa if col != \"target\"]\n",
    "y_columns = [\"target\"]\n",
    "spa = spa.reset_index()\n",
    "spa = spa[filter_columns_spa]\n",
    "spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1067cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa['elevation'] = spa[['datetime', 'day_of_year', 'longitude', 'latitude']].apply(\n",
    "    lambda x: calculate_elevation_angle(\n",
    "        local_time=x[0], day_of_year=x[1], longitude=x[2], latitude=x[3], utc_offset=2\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9726f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa.product_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spa[x_columns]\n",
    "del X['datetime']\n",
    "y = spa[y_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54576856",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_business = X[X.is_business == 1]\n",
    "X_consumer = X[X.is_business == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_business.product_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56858a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_consumer.product_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b701026",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = ['countyCodes', 'is_business', 'product_type', 'prediction_unit_id']\n",
    "\n",
    "\n",
    "for col in category_columns:\n",
    "    X[col] = X[col].astype('category')\n",
    "    del X[col]\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(\n",
    "    X:pd.DataFrame,\n",
    "    y:pd.Series,\n",
    "    config:Optional[Dict]=None,\n",
    "    n_jobs:int=1,\n",
    ") -> XGBRegressor:\n",
    "    '''\n",
    "    Train a xgboost regressor with L1 loss\n",
    "    '''\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:absoluteerror',\n",
    "        tree_method='hist',\n",
    "        n_jobs=n_jobs,\n",
    "        enable_categorical=True,\n",
    "        base_score=y.mean()\n",
    "    )\n",
    "    \n",
    "    if config:\n",
    "        # if config is supplied, set the model hyperparameters\n",
    "        model.set_params(**config)\n",
    "        \n",
    "    \n",
    "    return model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = XGBRegressor()\n",
    "# model_1.enable_categorical = True\n",
    "model_1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ndx = 3593933\n",
    "y_hat = []\n",
    "for ndx in X.index:\n",
    "    test_row = np.asarray([X.iloc[ndx]])\n",
    "    pred_val = model_1.predict(test_row)\n",
    "    y_hat.append(pred_val)\n",
    "    \n",
    "# actual_val = y.iloc[test_ndx]\n",
    "\n",
    "# make a prediction\n",
    "# yhat = model_1.predict(test_row)\n",
    "\n",
    "# summarize prediction\n",
    "# print('Predicted: %.3f' % yhat)\n",
    "# print('Actual: %.3f' % actual_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = spa.datetime.to_list()\n",
    "plt.plot(times, y.target.to_list(), 'b', linewidth=0.3)\n",
    "plt.plot(times, y_hat, 'o', linewidth=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts for the country,store,product\n",
    "desc_columns = ['county','is_business','product_type','is_consumption']\n",
    "\n",
    "fig, axs = plt.subplots(1, len(desc_columns), figsize=(5*len(desc_columns), 3))\n",
    "\n",
    "for i, column in enumerate(desc_columns):\n",
    "    _ = sns.countplot(train_df, x=column, ax=axs[i])\n",
    "\n",
    "_ = fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avgd = (\n",
    "    train_df\n",
    "    .groupby(['datetime','is_consumption'])\n",
    "    ['target'].mean()\n",
    "    .unstack()\n",
    "    .rename({0: 'produced', 1:'consumed'}, axis=1)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "_ = train_avgd.plot(ax=ax, alpha=0.5)\n",
    "_ = ax.set_ylabel('Energy consumed / produced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of average weekly sales\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,4))\n",
    "_ = train_avgd.resample('M').mean().plot(ax=ax, marker='.')\n",
    "_ = ax.set_ylabel('Average monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b641dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(6,4))\n",
    "train_avgd.groupby(train_avgd.index.hour).mean().plot(ax=ax, marker='.')\n",
    "_ = ax.set_xlabel('Hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8369a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5157f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train attributes\n",
    "extract_dt_attributes(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['county', 'product_type']\n",
    "for column in categorical_cols:\n",
    "    train_df[column] = train_df[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['is_consumption'] == 0][['target', 'hour', 'month']].groupby(by=['month', 'hour']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4938304-ed41-4ae9-ba2c-47aba4077e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_df = pd.read_csv('electricity_prices.csv')\n",
    "gas_df = pd.read_csv('gas_prices.csv')\n",
    "client_df = pd.read_csv('client.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87b2e3-c473-49d1-ac08-417772e603a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df[(client_df.data_block_id == 4) & (client_df.county == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ceebbe-66bd-4908-a06f-17d6ced23316",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[(train_df.data_block_id == 1) & (train_df.county == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d18a12-2260-4556-9b96-42ca7aae623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Observed Hours: {len(train_df.datetime.drop_duplicates().to_list())}\")\n",
    "print(f\"Counties: {len(train_df.county.drop_duplicates().to_list())}\")\n",
    "print(f\"Prediction Types: {len(train_df.prediction_unit_id.drop_duplicates().to_list())}\")\n",
    "print(f\"Data Blocks: {len(train_df.data_block_id.drop_duplicates().to_list())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe1727-5e42-4449-973a-91a0367dcb33",
   "metadata": {},
   "source": [
    "<h4>Business</h4|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e847df-4b29-4839-b859-91b6e496302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for county_id in train_df.county.drop_duplicates().to_list()[0:1]:\n",
    "    county_df = train_df[train_df.county == county_id]\n",
    "    for product_type in county_df.product_type.drop_duplicates().to_list()[0:1]:\n",
    "        cpt_df = county_df[county_df.product_type == product_type]\n",
    "        consumed_df = cpt_df[(cpt_df.is_business == 1) & (cpt_df.is_consumption == 1)]\n",
    "        print(consumed_df.sort_values('datetime', ascending=True))\n",
    "        x = range(len(consumed_df))\n",
    "        y = consumed_df.target.to_list()\n",
    "        plt.plot(x, y)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8b8ba-2123-4a47-b504-38d0c4420bef",
   "metadata": {},
   "source": [
    "<h4>Consumer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404aa4b1-82a5-47fa-bd52-e8d11183bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for county_id in train_df.county.drop_duplicates().to_list()[0:1]:\n",
    "    county_df = train_df[train_df.county == county_id]\n",
    "    for product_type in county_df.product_type.drop_duplicates().to_list():\n",
    "        cpt_df = county_df[county_df.product_type == product_type]\n",
    "        non_biz_cpt_df = cpt_df[cpt_df.is_business == 0]\n",
    "        print(non_biz_cpt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49082d1-abee-409e-a261-4588baaed23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
